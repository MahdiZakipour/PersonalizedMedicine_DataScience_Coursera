{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e641bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation: Apply some changes to image, before training(e.g. ratate, flip, crop/scale, color jitter, ...)\n",
    "\n",
    "### Wheight initialization, (e.g. Random initialization, HE init, Xavier init, ...)\n",
    "\n",
    "### Regularization, \n",
    "# 1-Batch Normalization, \n",
    "# 2-Dropout(randomly trun off some -like 20% each layer- Neurons, then Get answer, then repeat the cycle),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511d5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Processing - MNist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1348966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dfd375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABslklEQVR4nO39eZRlS37Xh34iYk9nPjlnVVbWXHe+fXu+3epWtyTUmtUNQoYlLEAswMBjWGBsg22e1+OBAVt+b2nxbJlnMzwhQMiyJQSNujUDPU+3b995qnnKeTjjniLi/RF7nzxZ062qW1VZavK7Vq3MOnmGHfvsHfGL7+/7+/6EtZZ97GMf+9jHPvaxj29nyL0+gH3sYx/72Mc+9rGP+439gGcf+9jHPvaxj31822M/4NnHPvaxj33sYx/f9tgPePaxj33sYx/72Me3PfYDnn3sYx/72Mc+9vFtj/2AZx/72Mc+9rGPfXzbw7vVHz8h/5Pf0zXrv2l+Sbzdc+5ojEKgJicQzQb5XItkIiRtKoYzksEByye+7zle3prn3KVp1IaPigUiFwgLGEjbBtPMOXJ4jVDlXNpqEV9s0HxL4nctXmyprGX42wny3FVsr4+J4wc7xrGxCqWQx4+QLrQ4+8kAG1gmvykJepago8GCsCCMxQqwSoAA4wm2j3roClgBzXOGiefXsZeWMN3uHR/K243xdsen5mahUWN4Yoq8qkiakv5BQTJlaJzaohamDDOPjbUGwZWAcEOgEkgbkEwZqie3OTm5xon6GmtpnQu9Cc6+coDaJUXrjCbczPG6KeKbr2Oz9J6N707GOA7vwDz99y5iPIGVAplZ/F5O8PxpzDDGJsnuF0iFrESYp0/QO1wlbguEgeqqpna+h33pDazWcBdWFvdyjMIP3HGeWEQ3A5K2T1aV5BVB9ygkM5qPPPMGocyRwnKuN0k3CUlzhacMVT/jRHONQ5VNPnPpCVavtpj7XQ9/aFGJIdjKUP0U3jyPTRJsnj/wMT6suCf3ohC7r6HiuhMHZtGTdXpHa8RtQecEqFjgdyHoWuQ1t1TWEGQ1GB7QyFQw8bKgtpJTudhFnLuC7nTu+fhue4wPMf5jHuMtA5597IYIAvJHF9k+UWXlw5qnn7zAj809R2o9ApFz0NvkJ6YSaiczjvmGlqxc9x6Z1fzbQYv1vE52QPH0ey7ywdDyRpbyWjrH33n9B+i8OsWxX/Xwz61gLl/Zg5Higp16jd7jU6w/4fHhZ1/hmeZFeh+NWE0bXBm0iLWHtpJh5hN6Oa1gSNXLqKiM9zXOcTVr80+f+xB5PSCrTzP329ldBTz3CukTh9g6GVL98SUmoiH9LGBSagKl6WcBUlgONbZ46sgLfPfHX+EJPyYUHr8bN+mbkNj4nE7muBK3WY3rhCrnve85zcHv2OZ4ZZV/evpZts5O8NiZBnptfc/GWaL/3kXm/5vTzIZdJvwBp/szfPXCYY7/D4dQV9fIl5Z3PV9WIuTcDK/9sYi/8d3/iimvR9+EfKlzkt/6zHs5fqaKGQxuOwC4X5DNOsxNc+YPNzGHh/zQI9+krhJ8qdnMqhgrqXkJ76pe5Dui88ypgKoMAHf/JTbjlUxxJp0lPuiTzHu0PjBkM6uyFDf52hvHCC9VOP7zc4jVdfTW9p6O99sdshIhDs6x8l1zbDxl+e++/5f5nuoZFlQVJd4+CZHYDG0tic35M+d/lBd/41GO/jLw0jUBjyjWwH3vuf9osR/wvA2E5yHCkK1PPU3nqGRwMqXa6vCBuSVO1laJjU/XRBgr2cjrSGFQWF7zOlRlwpTqkVmPvg3QVmKQnElmiY2PQfBivMiq3qZvQvom5Nn5C5yuDnhzfhb/4hGqV45y8JfPkF9deqDjtlpjhjHe0OD34ctfeowvzhznk0++wEzQ5VC4ybTfJRIpSli0FcQ2ILOKxPh8fusUr67PUns9xOsDxu7ZRCNrNeREm+WnI7rHDN2tButeFSktUrh/uZFYK1jp1tlKKmxmVX7Li/GF5lIyQWYUuZUsDxtsJxGDJMBYN4GeFtN8jhMM4gDbyFn68Udonc0Ifv3rezLeElYIPGGYDbocCjY44G+xUNniV/7au8gGi4j+UWrnFd4Qth/T2MggKznfe+plFvxN+iYks4q2P8BKHCuk9Z6Np/weL/6hI3RP5tQPbNOqxGykNTaoATDIfYyVGASne9N8Rj3NZNCnojJyI8msYqgD+nlAatTo+++qiMxKPGFYOLjBZqvCW39ijurVeWaeHxKcXSG/dHnPxv5tgWsCDjUxwfk/9zjxrEHMxhybu8TH25d5LLhKZuFcPsAXEAnBuhYMrMe6rhHJjAXVIyz+tm3c+/kCfmzmOaIfzPjioyewqx+idlHSOqup/vJXduafaxmmhwX3ICDzFg5i2w3s6fNYbVDzs9heH725eY8Ocu+gHj3J5R+YZfL1jMrpdcyFy9gkQXge1lgwbz83PTwBjxAg5G0d9IOEqFSQ7RbLz8JT7z7Dp2afp6GGKCxbukrXRPR0RGx8hnpsspUzhDLnULBJZhUbeQ1jBRo5WigBtvMKp+NZWt6QUGa8r36ODzXeoraY8qun3s03rixi/n0blpYf7E1qLTZJkKlBxZapbwkGcxEbp6rM+l2eqlzksWCVGSloyoieTTifC1Z1jdW8yT9ffz8bl9rMnzcgQAdvy6LeN8hGHX1gkt5hQ3S4y2CzQipAhtrNMcKCFVgDVkvSzGOQ7QQ0FjBWYK2gnwQkiYcxEqzAaIGNFXKgsBMpXpSz+QHIawEHf9u76xTQPRu7MNRVzEFvk5pM+FDlDH/+Oz5P1yjWTYW/+sofYnO7xj949p+x6G0TCUPXegyMz5auklqPhopBcEdpuvsylkad/OAk5qPb/MVHvsTLvYP0dUAni0i1QltJbiTaONZxkPgkcYAf5HieRmuJMYI8VyhlUMow2+wReRkDFeAJTaRyHm2vICcsm7MVXri0wEZaZabXhv2A555BhCHMTvH0D7/GD029wPdUz1ETEl9INkxO30hWTZVIZNREzsW8zZaucj6dpq5iiC7SFAlVmbOhK2gEkzLmo5WLfHJxmfMHPsv5fIK/d+YHufL1A5z4bBWbpjvs5MMW9JTrHwDmzo5NKoR0r9ezE8QHq9Q22thcky1O4a34sLX1cI33dlGcF1mJGJyYwPveNVYr08zmE0TLa+g0RXge5DnWvP3b7XnAIzwPWa1iTi2SzFSpfPmN6ylkqdzPPQiGOj/wBKt/cMjvO/Yij9SWuJBOOabGCqRwF9BQ+2hksVt0x6gwGCs4F08BoMf04Yrrv5mujujqiO28ihLu7083LnPi5Bq/8fh30oqPok+fe+AXrZUC4wmENlRW4WufeYovPXacn3xSsaWrtNWApozp2yoX0yk+v3WK1zdmyX53muktS9IGmYI/tGBu44q8Dxi8+zAXvl9h6ppBJ3LCImkR0qKUQUqLMQKjJVpDMvC5NJzAxgqMAK84buNeRxm7WUALMAKrLGz7aAXV+R69I4rhD76X+ssr5GfO7cm4ZWa42JvgWHWdOPBZzZr4IucEq0zKlEPBkPS3pjn29QH/2+GP871Tr/KJ2uujoBVcoHdmOI2K9y5gLdH90BEuftLwSGuJl3sHudCfGN2HxgqMFWRaoa1ACEvgaYhStJakqYfO3TwipcH3NaGfYawgzn0y4/7WFRYlDFJYPGk4NrvOxo8O2RxOMflWA9Pr/d5cOB4GjJ238//1+2h9cIW/dfCzVEXO+bxKJHIioYmtSz/WRIrCklnJEW+TU/46740ukVlJZt18OjAeVZkBsGEito3lijAoPGZUl//lkV/gV+ffzf8+950c/hVJ9OmvPvzf3x0cnwhD9AceJ5kO6B1QDGcFyaRB/MQB/CAnCFLE7x7gwD+4gk3Th3/ssIux8Y4dIT46xcpfHPIdCy/xl2Z/h/9+8of48mPHeeziLHS73El7rAcf8IxF1jKKEK0m5uAM/cN14glJZXrS5W2twcbJ24p27xukQtaqDOYU3338TY5X1qjKlLWs4XaRSEKZ4wtN04tRwuAXwY4UOwt7Ynz3GDtfii9zFBaNwFhHsw90gEYWKSH3tRzwt5jxuvTnFdWFNvKMBPtggz5hLcJaJ3rNLbWrsF2v8JuTjzFfO0jLj5kM+vR1yIX+BG8uz5CtVZhedyLsrC6RuS1pkgd67CWcoNoprIW0WOGOx2SyiF0MRruUlrVALsEIRCbdZgvpBNq6CGzG134LwojRTyvsrr/tJWRu2RxGDHSAEobY+GRC0bcBk6RURUC0YQnOrnChM8HVZhtfgLaSvgmpyaRI96m9HYsQCM8nbiuOHFqh4mV085BMK4wV+MrdE540WCuQVqCERUmDlMYFQVpiLUhp8TxDNUypBSmeNLvuTYNAW4kuHqp4GacmVnlpehoxN41I0+vF3vu4bchqFdlskD0y4I8e/QozKiWz0DfhqGZYI1BYVPG9aASBMPgCQiwZmi6iYMwFPgZd3JQagbYKhCYSmoPK8tH667z2xBwvfuMpatNT6I2thy6T4CYeM8byOKhmE1GrYnp9hBCIRh1br6InqohMYwKPtXdXSZuusELXNCLSRJUUISAeBtSyvRnSLTHOaNkxRqtI68koRIQh3adn2Trp8YdPfIEP195kTrm11aZy5zXGugDpNvBgA55ykFY7bczxw3Qfm+Dyd7nFRKbgDecIt6dRiSG8tA2vv3Xji7NgfYQs0g7jQsqSEXoHkLUq+btP0jts+XDzNOeTaVbSBqHMUdKd9LpKaKiY91bOMiljppTFRyCFYGA0GpdjBsjKvLUQVIU7vthqto1l3YQ8NzzKWt4YaXvAsUISQ//DA9J2lcNfVNgHfKO6lBbkVfcdVVc11VWwX5rh3NQcWU2QtkAlUFmxTPcM3tCSNixZVWAV2PIelnvDEtReX+WwmOHCDyka0326nQo2UbAVkIcGfAO5cJERuHknF9iaRnjGxeiZhKFC5OK6xV8YgchBVw14lsFGlcY5ReWzXyXfQ82LGuZsLjVZnmkQicwF1EbRNRGxHFCGe2jN2lqDC1OTHJ6pcy7PiK3PlOwBMBn0sXvIBQvPRy3ME08Jnm6usZ7U6KQRvtIjVqfqp0Qqxwv0SJcDkBtJbhQGQaoVgdJUvZSGl1DzktHfEu2RWYmxkq20QqI9hrkPHsx4Kb3jmpXvnmf2X/fRyyt7dzJ+r+PkYVbfO8GnHv0Kf7z5Jl9PmqTWzYeRyKmK3AWdRUADLohZ1a4ApNxMqrGbMLOOXZ+U8ei1mZUMjM9X8ioNOeTvLPwa3/n+R6itnKT5u2+i1zce8MBvA9Zet6FNPnCKzUcCpl6KQcLqMxW235Xyh973dV7vzJFbyV+Z/xrbusaXto7z4soBeis1Wp9pUL8wRL3wFjbN9jwdPR7glOk3oSQotVMJKXbWcnl0kf5jM4R/6Sp//+i/RQpD10T8au8E3/yNx3n8l9awFy476UWe3TZz9cCnMaEU1hqE5zE42mL7qKJ9dJ3+MCQb+nSOhvh9d2LCgzPUFltU3lrFrG/urvApEnbWFCcxDEcMgtNNvLP0iahW6ByNyCczqjLBF3pU5gqghMEg6OqIn774g3SzkDj3iDOPJPMJ/QwlLcPUHzEHtriBPU+jhEVKQ8XPaUdD3t2+xJzfIS4YIV8WqTFheWbxEt/oH0MoiX3Q0bq1CFPEAhLyULoye+sYHJm7YEdqUKljgvKKK01n77MgANjtDtVzAdHSNJ1qjfrEgMT3yYeRe4IAfOuun0witHBsTipdoFOyOKZ4rgCRFSX4oQGNO0HKghG0X/Ronc32vpopyfE2qnTSythuWdLVFQbKI0eTNgRmbhIEZEYxMCnGuvMii9RrJ48Qe7ghFkpiqxF5BQ5VNt0Cl4WjwMYTBk9qfGGoqGwXw2qKaNsxN475qciUisqIZEZmnfbHeILMKjKrqHkJqfFYT2pEKnObnMmE7uEKc5Vor07D701co5XpPNam+8M9nqldoGtyIuEmtH5R8JAVRR8KO9r4BRi0sCMWR92AbjRWYIQYPaeExKCRDKzgI4+9xRfVcepnDyDjBNPv3/AY9xRCoGZnEFKClGweCugeAeNVsBL6hw0oy29deoSNy21UT/I3vSNufhYWoQVeKvAHGplpRL0Gvf7eBzwlg0UR0wk7YnZG7Iy1TuLSaLD93jmufF/OT02fpioTjJV8a3CE/9/LH2LyLQMr6zvz60Ob0hJytMsXQcDWCZ/eCc0fOfwKb/VnWOo3OZ9PQyYhNBBLvF7Aopyl8qrB9Ac7bE85SKtd+qladSfQ2KJ09p1dwKIS0TkhaM/0qMmEqkpGE2KZvtJWsplHvPKl41SWBNG6pdbVTK5nxDN1tC+YW0kdIyBA5BZhLCYIMEqQ1xSDWcVbhwTN7415fPoKy1nTTeRjN/VPzn+ZbhqBeufM1R3DgtQWYQVWQlYtJxSBP7R4Q0O0ZTFKoENBXik1PwWzYxn59exVSkuvb8D6Bo33TZFXfRaPbbEZV1hej0C5NJf0DUYLSFQR8IAauMVSZgIrwIQWW2h4ZBnwVJ0HERYXNMWCA7+xhF1avYFS68FCDlIqK4KtpIIvcseGWDUS2w9Mj2QChofqSOX+3rMZqa2hMARCMwC2swpiL2M3pTC1kLxmORau0stDlLDUVEpFpUx4AzLrmJq6igvdR6HXKRZQKQzGSuRY6llhyArqqvx7+brMKt5gDiksocw4NLPJRcBWwj05Bb8nMcbol9h4QvLqR36e01mPZe3TkgnSGtazOrFI8a1GjUXX5QZTYlFjjM84yhRYqesp/y6FISr0PUu6xk8f+jRqUfAHPv2f015quYCn8Bvb6+KCEsLzMQdnsKHCKknvsEAd79I56COkpVaP6a7V6D83zYFXDPVLMfLrr6Im2sRPHmL7WEA8JVCJwUqBPTCNXPN3gru9xPj5tdpxEtdsCoXnwcwkyx8QfO77foZlHRBbD19ovrp5lMlPV5j85sZd23482IDHaGxa5OsCn94Rw8ThTT5cf5OWN+B8ME2iFevbNThXI69Y9GzK+R/w8b5zkeaZRcKOoXo1xlvrITo90pMHSKYCOkc8wk1L7WpK5a1V7MbWOzvUVo36+9f4wNwFMuuhsPhCk1mFLzQtNWQjr9HTIfNf1lQvDeicqJM0FIMpxfQLPWRnyOp3zCA1VFdybHVMBGwsXl8z+UrKzFcTXn5mnh+YenF0g9ZVjLGSro54OrzM4+0lXj9xEnVl9YF7vNgySLUgCpbDCsgjAaEoi5ywcix9NT4nCTCKPUtplZj+3GXqV2ZY+H3bzFW6rE400YnCphKdOt2OTAQyFah0J8MFbjgqEVgldh6XIGIFnsUGluh8QLQKbGy54HyPIeKUaN2yPYyIrY8UFh9NVSZ0TMQrWc7v/4OfZ/lHmvzBxkUmvR7PJ202dB1f6NHueD2uofZwgyiqVbZP1MgmNL7QLEYbTPl9IpkVzGvGwAQkxufscIaNtMqZrSm6g4ikHziWrmDt0EWwaorrVllMZAlnB5yYWeN7Z15loEMS6zEXdghlzrTf5Xw0yXLQ2Ckd3sfbYyxF4x0/ypt/+gAL77/MpbzHkq4SW5/3Bl0ik3ARilSUCzhLjSN2J+i5Fcqgpwx2MquKx1xQ2zERW3JAJAzbJxTh9kH8q8tuTdrjYEd4HiII4NQR8maEyA0yN8hBzOQrIb2tBtmzQ6q1hMEgpP56wKFf30BoC2mG0RrT7RGdXWc4PU88Ldh4LEBmPjKDidcC5IOuMLwJa6baLUQUYRs1CHx0LWB4oELcVmR1gQkgjyA6sc35vMpryUG6JmLe2+bs5iQHXuvC+tZdH9aDz8xbiwhDRL2Gmc5YbG4zrzps+TUS47PYqKONZJMa1jeE1RSqKTpXdKgQbimMV6EaKrxGRPdIRDwp6S1a0oZABwHBVhOVvzMO3oQeH55/i1OVlVGOWQnjKjiw+MIJlo2VVC/2UVfWSd7XIG0KsjpMvipRWU7/kNN3qFSRVQQ6dI7EMrNU1gXh2hDzwmv0B89gkG6hKXahjm73iYRmwhuQTteIukN4kAGPdMyOFQVLAyPGyihumrayYudvLhgSe75Y5OcuEGnD5UGLup/gBzkmky4tagodWS6Q2v1ejtd6LqKTeUnHgvEcsyM0Tt/iGaJ1aF7MseNM5F4i1wR9Qy/zRkyGwqCEJbY+q7rJn5z8Ii0pWNaSdVPhXDpNZr1RVaG2gmHmI/aYrtK+QGSC5/uHR8UCERldHXEunqKTV+jmIee2J+kMIoYrVbyuorItkJn7nsqfKnVpWmFc6jWvSAadOq+lHu1gyHTYo64SWt6QqkxpyCEtPyYKMqzc78ZzN9CtGosfuMzvm32drpHE1iezHlXpE1snAtdWooVAck1q6jZjkV3C5bH0VwZk1iNDEmBIJi2DGY+2dFYUe87sCIkIfNLJCsmET7iZYXOJ8CTCgDe02Fw6O4z1kPplg3nhNdTUJML3XcCWZthuf3Rtpy1Gm7PKRkjND+5I6/LOxrOb2SsrsUWjjplqoqsBWTMgr0iSpmJwQBBPWvKpzFXEasHJVoelvM1a3qCrIyKRMYx91PoWZjC860PbEymiee9jbJ6q8sFTr/Oe5kUu523AVSW1JoZ8qK3oHop4uXuAV1fn6K3WkAMFPsTTlmRCsFIJMJGPFQY1hNoFyWDBkrx/m+VKm+b56js6Rl31+cmpL5JZxel0FikMocxGFDlAyxswF3a4oCTpyQP83f/yHxYl2gF//vE/QrIxyz/8xP+X0+kc//PrH+dHjrzM729/g0holvIGv7D2Ib76r5/m0LfAGkFqPSY8Rz1mVo0o9q/Fh1lOm6w9HTJjp1Bnz7+jsd0JTCDJK85HRxiQqUvpWFmwPbZgdcQ1jIjdCYisJ9Chhb1aLMZoa720Av/Vo5x9qg4/1Bs9RSYSod2YrAQdWlQqdsaHRStG6S7GAjr3GTD1Uoz/5Vf2rrLwGtjhkMpySjwIXBVMgTI127WS38weBcAXemSHsKFr9HREUw7pmspt7bDvJ/TaGpO/PGD6CzM8/4vPcPWjVQYLmsZih+6lJod/zVC50keubTOle0zZLjbPHT3ueRD4WE9hIx+rFCbysJ7EegIZa9QgRVxeAaXYDOp88b8/zh9/5ss8VbkIwJau8UhtGTMruBoe3tNz8XsVuubzpxc/x1F/DYBIZEQiQyLRFPNdEWQbuzNPGIBRCtJel8667nPGAh2NIMVtVhtySFvmtKTCO9aj02nQvg0X53uOUTCwU5kkohDRaIzS/9vHQuJpQe9kxve9+wV+Yuor/Ll/+meZetmj+bmzruUQoDcKM0FrkZUIZicB8LsW7QvSScOzH3ydr9Ye5eSFR1CnLz8YA0Ih3XxrtKvEPrbI0ndNk32fMwoNvQG9JGS62ueHp9/kctJmK6vwqann6ZuAf3zho7SCIV/tH8cXjtXtmghrJNZTiFttnHf5GV2PPQl4tk9V2XgCfqC+zAF/EyUMNZk4kymZoJFMex0S4zHIAy4ISzwMnHFYUS2DxHmiaIEJLYODFntoyNOzV3nVtAk37p6DV+0Wcd0rAo7x3bGF4guIZEbfhBjrmAA1SPmZi5/Ak4Z+FpCuVPF6kr/51ifZGkYM3mrxfw7fw5emjxFITWoU6/0qQeF+blLFWtZg2t8RZuuiRLZvQiSW4Zwlafu8s1DuzmCFKNgZRv/sGHMDux8bsTrsPMdeGxzsAWzh9GzzDHVuidrMUdYHvptzvKIc1BbBTHnwBQuAV6bz3I7JetcEANqltrz+Htoo3AjauMVcBsx7W646y/qk1htVwDg2ZycNIIUlMx6ZUaRF0K3t3n55slrFPHmctB6Q1xR+FypXFV3TorIkCbZihgdrZKcaQPFdjRNsxTXoeoqB8QXG2wnaha7iPdZCpRaVGtSSxz/zPsi/qr2L3EjiYUCeKMTA47HtFR4C7u7t8U6EuPepBUO5eCmxI6AHzbgyUSNRxRlOUc6zzCqkMCMbD9ht/VHCWHldQFSK0pUwaAvGWh6fW+KbByt7UwRyIxgDxhT2HzCYE+gI1LbH77z1KN9aW6D9pqF+todeWR3Tr459P0phqgHGd/O13wfrSd7ach5aOvJQRan3fYc1owXBaoMcJkQblu3zTfqq4TSQiWAtmuStyRmyoQ+p5BszizSrMU9NLjETdDkQbI9kI9/cWkSvRoj+EuZGla+3mT24vwHPtQdhLQjBykdyfuKDX+Fj9ddGKv2SNXETsmJgQ95XO8f7aufYmqsWu1KPN4ez/PtLJ+ms15DbHkILdEPzsY+/yGP1qzwTXeBvrj2K+MLzd33M9ugCg1nF6WwWhSG2hZeOMPg4IVxbDejqCp284tJ0r51D/7XjGGMJcsPR6RyrBNX/JaORDYANpyrXBqSg2mqSvq9J42zHZYj6ijcHs7SagyLvrEauzT0dUVEp6lSP/oX6gw14JNiSqRkPcOROimsUAI3pd0YLzpi2x+6Vhme83NNa9Ooq0fIsarlJPpnh11O3oGkQuRwxVzIriglUIWJOIK9BHu6wW0hQQ+kaxSbZXlvv7IJNEtRGD89v8cHQcjlP6JuQrg5G91u5CIFbbOKiTFsKi0ESW49MK/ZSgS0OzvHmH6u6QNNA+xVonTG03wKZafK6z4U/ZPjrz/5bwC1yl5MJVtMGy3GD9WGVOHPjsGPBmxCWyeqQ6UqPT848z+lkjhe3D9L7pyeZ/oUM2c8QgxizsTly6d3zYOfaYOQGjMHbCnHfJhgSnpvv7mUKRFjY0lViz6NBNvIcY+yMplYhje/uqXIORKKFC3p2BTSW0XULLtgppQfASAvZNRUkhppIGFiFbzV/ffHX+Dv2R0j2pAjk+tJzm+fYOAbjgvHBIwlq3efg5wz1VzroN88Ab91ybhFhQNoKSGtONtG4qLFXBdvpDI0Vi/UltlFDPQiW3dpRBZXNUvJzF2itrjPx+RZmfeOWm0Lz8ffwB//RpznibVKThl/uPsWb/Vle/MYxJl8WrsWSEM56pqzEFnLHnkbrW8oJ7m/Ac5ObRWSS9azmevUIj74JiWSKjx7pZNpiQGoVGYoGwu1EreB4ZRW5aFmZabCVOm+GdjDkhyZf4GI2yb/afB9+/+6nJeH5LH20zdZTOVOq54IdvUOz1lTivHFMSGw9DILBoSq17DDJVIQJRKELcJG2lQeQmUFYkLlB5BbrCfKKR9qQdI/VqDTegzcdM+n3GZiASOS01YC+CEmMT1UmzAYdnj5whZcnHr3rsd0VpGM1RM5O5dU1TM91d2JZlWXd6+SerxI3gCjG5RvCMCcr20soV5FWpq1GYzcgc0ZjLVNf4B73eyCyh2ugNk2x65vkmaO6XbmvIRJ5kZo1u9JVEo3CkAgfLeRoQenHAWG6h6LOQUzjTUUyZUmmNd0jlt5hgVgckHUCGq8H+Jcsf2/7k9jQgGfwqjvVHzqTWFMI7wWOGTbOKqLfjbjIBN948yhiqFBdibcgGM60UGnLuYT3DjOYF2RNy8l/toE9d2nvql5uMKcKKXavodZeF+yoiQkGHz7J1gmfzinNIz8/wH7txRt/RJ7d9LPuGELgHVpgayHiaLBKTWR0jT+qAkxshgYaaoi2koENUdZgkAX7aAhgjIX0RnYJcH2aq0zLlsGRxGUPDqougTBktkyfPTzic5vlmF6f4OwK/loVbziB18+Izm3AVgdxO/ob3yeZcMu5N7R4Q4vMNFOvSucRFkj0ZA1Z3ZsqQ5skmK3tHb+da4P0AmqQ8y9Wn+X7J1/mh6oX6emIQe4THu4xXGsWb1aUuI9ea0bXv6xWEbcIZPckpSUywXZWYWBCMqFZ13Vq1qW0ysCnJl0UaKwkEhmB0KRWsSg3OBqsoRtypMxvyCFPB5v8ct7ihfWDhPHdb0eFkmw/ojl6YpmqTEi1o1XL/HJp4NY3IZlxQVhvXmFFk7Th9C6uNNu9X9r0EdqORJNSg/ad0DWrC9KWondQMTOxRNOLSYyPr1wljbYSJKOqsMcbS3yr+sg7Pv93flKca+94hdZ4mmqkaRlDGQzJIkW0x1mRG0JYEMoS+TmlmscqtwGTZmeMQhfjKFJcAnaCOlzVj9cH3qFQ/l7D5jm608Hm7totWyb4OD+p8RYnpUjZeduYwnrBBQ1J4lPZw7J0myTUlgwmlCSzYOdjKvWEv/jYv+NrnWP8dvoU9XOKiVctWU2RVz0G8z4msNjQuu8KsKqwEFC2aAkC1nrIoaB5UaISi0pg/f2aYGZAkit0ohA9jxNPXOHZqXN88XefJVoK4WEo870ZCoantAERngezU6y8x0e9f4ufffr/4v/xlT/BxIvRjXfbBRMv/OBtd8y3Az3bYjgpmFc9JHbUIyvAkFmDwbWR6NiItEhBIUr3ZInGjBovA6OfWG7KuJVyAHDz55SyxBbiG/Qz3HMYjU30qDlt5VX3sAZX4BMUwuRbuez7HllNgnBMtEoNXi8jupygmyFpOyCv+YjK3jiI2jzfFewIpdwG89qy9GHGN64scriyyY/XlxjogNR4nJxZ45Wp+g47eU2gJDwf4XvIdgsC/6bHsSejt76l5Q+JZIbEEIjcUe02Yjlr0dMhy0mDA1GHY+FqcYHu5G4VhoaKiYQzDstQZMC8t827pq7wSjR11wOz2tA4rTgv5/n55kc4VlnjZLTElOrRkDEzMmFZV/jy8ASJ9aiojJ/8C79OWJTIuioYd4O6fLRbVHyRj27UMqccCacDKkWiifGY8PtUZUokMmLhE1uf14YHON2b5vkXjzP36oPdadsRm1OUaRVzzYjhEGOprPHXlexHZskjga6LW16IDwRj6QA5zAg3BP1JhSjHJYqgtGByrIcrPTeFADtwj7vSdfd32zDIBKprBhEXurE97P12I9hc0DPJaAG4FuUCUN5nmfFG5cHGSvTAQ+5hWbpdmMX+1CqL0ZCql7IyaJAZyb/beBRPaj76/ld55OMrHA7WCIoqRzc3eEV58o4OySBH56G8B8tChHVd52ra5uXuAdbjGkvbDQadgMZpxaXVRS4EhzhxZgm9ub13J+NajKUPSniHD7HxHQsM5iTJpKX2nnUWmh0+HC4xHfZ4M5mn8wd6rD/9Hk799OvXuQ4Lz0MtLnDxDyww9UpG8Nmv3f3xCWegZ5UgEnrExjRERkOaIjBxTw3EDsNfmg9KiiKRYt4f/+7GW0+MHi88lwYmLFpS6EKWYImtc2B+2h9wsrHK6zxknkrjDbTLwKCYs4QUWNRN5xQb+iRtV0kq0yKwFwKUwASKvCqLNP1eu4MB1mDH2fAyBZvnyM0Owe8e5d96T/K3Z1/El5pIZXygfZ6XZg4iq9UdZ2aK9lQnjrD0nZP0Pj4g6/vOx+8meOCtJYRSWN/Q9l1p2fgEpJF08oitrMryoEkoNcfC1VFVifMGcbtQbQUZpUmYx5IOyayi7Q3Iaopqs3l3x2gN9Ssa6yl+u/Eo9dZhDre3WKhuMeX3+VjjddLiWMrj/njtNWZUSt/IsbYQO9UCvjD4xc2pBGQlMwB0rceGrrrWEqbOZlbj7HCGM933EOc+iVZsdqsk2xGt1xS1qw9IFFt+V1LsYmd2laff8vVAKR4VYAInontoXE2zHL8H5HKU1hl5CYmiR2ixkSha84yMFMuS5tJ5WRi3oyqbowopbru3ywOBFWRj1L8e1cZcD40Y9ZMCF5yTS6Teu/FYKWmFMdNRj7qXIoVlkAcsDxv4UlP3E6oyZcHfdMyr9eiaCplVI+fyEhpJYvxRkBMLH19o2mpAJFJanvNPMlZwqL3NeS3Jag101TorgihERiFmsPc+S8DIw0VOtCEMMI0K3WMN1t4tyCcyglbCjxx+mYVgk7Ws4XqqWY/3L1zgeW8B4V+/CVFzs6QLE/SOaiqrHsE7PMa85pNXwBeAtRgrCYQhEILMWrKijLzkcMbTVGWKatfx3cQjoXx8nLksAyaJs1jQCHwhqKsE2Z4FrR+uQoNrYItehm8LKdGhawFkpSDseCAEqhvvev0eF1zeRFO2E6DYwZDWmYzz63UGJkViCaSmKlO8IEfOTGH7Q0gSzKlF0lZE71DA1pOGP/DIi3z6radIN27uhn7/RcvlYIx2N2a1StBOeLJyqZicFAMTEkpXppgYn74O2E4iZMNwNFjjdDpLYnwaypmN1WTClq6ymtdYDNaJjc8vbjxLKHPqKqG7KKm87+RdHbLNcxq/8g0aRa8PUa9hG3VOH3mcF+cD/o9PvpdHDqzwUwtfYC1r0M9DPtN9FwAX4wmUsASFT8jNynlHrI/UDHTAUAfMBF18qfmNK4+x8dwsx//OC1SspQK0AYzBauOozQcAEQTIdgsdyV26nVF66iaVV6V2x6qCECrkADqymMhHhuFDMcGIYUJtSdM5IV1vNM9gQ0EuweuC399xi86aFpELvEER6GjHAhnHuLv3y9mzbvC3g/SaicZYd28q3M4ZIYsqlp0dtLGSxHrIWI6+x72A2u7z5lcPc/rwkA8cOc93TJwmEhl//5XvZrheoXLJ51v1U+jIEq1K55dUBq8Kl9pSu9OqpYmmTMSOHYHn7AjSSY0/GfOZD/8sl4/V+dmF7+E/mfk67w2v8ONv/pfM+Aqef+WhCNzlxAR2YYbLH2vTP2RpPr7O++de5J/N/ybPJ7O8lcxzIZnk0nCC1aTOoeoWj1ev8qfm/gPfah7hs9Ezu95PeB4bHz/M9nHJoUevsnnhgJt/7hJCKbaPhfQXDQ0h6RZznyz+9S10jc+6rtOWg5Gtx66Auwhkdj0GI0fmcXan9EdzppmqYPFyJmVA1+Rs6Co10UEJQ+89h6i9WXX9Gh8GXFNcgXWpLgtv2x/SqsLj7aku7164zFe/cYraBZ/FixaZaLyBwRvqh4PhuRZ2R4OjNzcJP/sc1Xc/y9nv0VRVwnTYYzlrEkUZG99xkHBTo1JD+Deu8mNzL6IwHPQ3Oext8tuf/xAnfmsV/syNP+q+i5aFEuWYUPOzxCdmOTi5xoK/ybquAxDKbBQEVFRKw/OYrvZpejGpdRqaUO4o+7V1TTWrMmFS9bhopvidS4/QqsQcqm8hNBh19znaHYU5CG0QSUoIeN06nW+1eC09gFrYPdlJ7KjX1s2CnV07j7HnGcTIaHB5pUVjTey9FbixkOfI3I50ODcSJ98Q42XrBQMiU4HIXND2UMAYVOJ6z+zK549Vno07RwvjcuM6dI8Zj5FOC+EW1j3zGboN3Gi6LNOvUKRZMbvKfR0TZDGRwfh7NzbrKbKmphLkDPKAzbwGwHClihpK0rZBT2ZE9ZRhI3TMmwSEaxsilHXZzOKeFBRFZ1aQFU1jrRYugjWgOh55VuUbyQIbeZ3lQYN/33mUV4ODVNYNarvPfZM0jac1bvaUMERNTtB99jDdQx7dowZ5qMdUc8Dh5iZ1lfBiOs1S3iY2/kh8vjqoUfVSsori51c/wnMrC8wnHccSVSqIxQMkBxosfbfGq8Usf32eiSWDd2QRs7x61xsVW1iISCEK3Y0gQ4waKoMLtJ3GzKCNLExYd5/lUsc5ns6SRQVXCYUlEnokem7LAaHQ+CLEoAt2Ho6Fq/zSezyEmSB8/a6G9WAxqkgan6vGmBvt9Gf9bsjFbhvaGUlPOkNCVSFpKxfwpA9Hmv2WMBqVwOlsCoVlyu8z7Xf56MJZfutHHyWLPcglf3b6TY4Gq2TWY0PXeTVeoLJuYPXmjWHvf0prjK7KDk6y8t6Q75u8yHFve2SGVpOObjRIRzUKS81LmPAGDEw4umFLXUGZmw2EZkH1uJhN0X1jgq1WTmc+dPqKdxDwjMNmKTZLMd0u4lLAgv8Yl0Ud/yPuZjRWjPQ7TS8eOTGPpwXKRaXsj2UQSCx1FTufHUKqKqGnI7zLIdXlhyAosAabZq7/l+b2aFVxg5ioEP2qGGSSofe6iV0JrZGpQYyXKo8HdWXA47nHRA7ewDVHtco9bvyxVJgSXGfD8LBAWNQ1x7bTo8iMfFFSPJR1fdx04b+jhEHWM/Jw75pm2sAnnB7SrMYMc58rcZt+HlC96KEjMI/0OTG7zqOt5ev6ZcGORmlcB1iOP5TuPs6NZDlpcrnf4uoXF/CXBb+99QRD7XN1s8lKp06eK46d62OWV7mn7M54qbmQCN/bacFzg+fKRp3s2BwXP6V59pG3+OsHP8O6qdI1Fb7eP0ZPh/zrjfeMHKnLjdhmt0ozSMiait965TGqb4TYeBURhsipCTaemWTjCcH/9PF/zqvxQT739z8IQHJ8hmAQwzthZgWosUqrzEpi9A3dDjLcpra0LEmtGmmzyvTajXx4dLFxrIkcjSA2PpGXEwmDLD8bRSQET4SXaX54hc3OLPN3P6oHh/JaGGd6xkXMRYreW/O5qiZoT/bYHCpEnGICyXBaUl121hkPDW4SvIHbXL40XCSSGdN+l1PBEt9ZfZP/z8EvAmCwvJzmdGxIbHy+Hh/j3y2dorqc3bL10v0LeIqdynjncm9rQPNChYvDCQZWjaqvYEfzctDbBGDLVImNT8dUXCltEVTUClZnQ9fp6IgX03li4/PUB8/w0uUDDJ6fZPGFAcFbV+/NOKQqBFXFhWJcALCUt8mswpO68MuxBVMgkaUzqDA3FYqWUEWn57LhoTcQePFYNP82u737BWus+0ZuIkq+XRh/1wZsbzF2U1lrEdriDQUbnarrkp4JvIFAJYV+IAESELlzmTaBcBUQCWQNJ+SWsRMKPuwY/wpLplEJM7r/xr2mVNF0MSua9s1OdRg0Kg/6kEewvuLo9AbTUZ+al/BU7QpreZ1Xeo8jc+inikR7JMZjO6tgbOEjZMXod2D0fzdO91jDT5BYkoKuW6htc5UFVAptf8iU3+fyZJuGn2AQdGYXqS1P3FsGttwUWr2732ABNT1F9tgiW6cq9A8KeE+HhYkNfrS1RF0lfLr7DAMd7PT584YcCLaJjU9mFT0dEsqcDx8+B8Br/XmePn6Z/mLA5WfnRzG+1l1spvhb//NP0rikaV68CL6HqoTOJ+ZuYA1eYpHFPSVxBRxxMSnEVtG3wUhTNd4JvWzoWjI/N0M5z8bWRxMTjq2j1YIluqoH9G2Ijy4EzC4YGjwsc9Pt4pq1QHge8tQxdCOidS5j8lWD1IaV90zRULD97AJpXWI88LcTxJXVPTrwG+AWm4YDv77EZy5/F/0/scV/dvLzPBFs8ivdJ/nU574ffIOKNP/Fe36Dp6OLTKk+b/RmWXpplpNb3Vv6Fd3HgMeZAe2qIJASHbiJp7zgZeHHUCrtZ1XPMTwmYVU36WaVgjUxIzvyMvLPrMd67tJin5h5hTfXpgmWq/jLHfKl5Xs3ljHDPKENMoNtXSk6p+9UCux6ScHy3ArljnN8NyozV9k0+jwpHAOxV3qBm2h1bve1pQj4oYUBoxXCiBHDU6asVClOxj2WV0CmjDQilN3ghXPxfWgZngI7aQB7TXp192IihUEWVYa+0MxVe7wVzj3QY90FTzId9ZkOe/hCM+n1nCVF4sqnrRHkxlWYxblPbiWeMORWkhuJJ0tvlt3pSymcIFIKS6w9ql5K00ucKD135nVVmTIV9ZkO+2grWK8dwVaj+yu+txZZqyGCAFGvkS9MsvFEhe1HLeHRDv/NU5/lqL/K6XSWq9kE5+MpMuPm07mwQyhy6qqw9TCCvPjb4coG62mdlaTOqfoKs0GXx49cZktXeXGwyHMbi1xcnWDuqz288yvYyRbWV1hfuWqY4V30MBKSPBKYaPc15hqFypF7clldpa5ZrqQw6NvcMWXWK95rp1orKq7tbnF9ONG2uwamKz3W3qkiew8hazVko85wwRXoBBsp3moHtns0po+TNiRp3a25Mgc5zDDd3tu860OC1XUaL8HK0K3vL6VTfGblSWa+pNChIq8FXHhiiieiy0RCszqsU7skkb3kluagdxbwlP4OAMVPIVxQc52rp9GjtGOJy983xV/9M/8HbTVgSTdZz+soYV07iWLx3zLOR7hU6pfmf669Q+pcmE3o/BqQBCJnUvX4nsoGv9R4H4NeE5Hdwy332I7LtSW4SvWRBhfjyWLCzBkYd9eU1S9lykpirzv5TgzqjK+6OsIgqKq0uLEl3gDUsHiVkMXu78EzPEIKZ70udgKBcbO9Xc8t/Xlu8D6jTuvltbCXVVpjqQPh+2QNj7xmqdViOonC+oK0XYp4QHY9RA4msljPIiLttB66LOMCpCXWAWooaVVcivbaMuE9x1g6YZzd2WXgNsZG+kKD3EnXfnDiHK82jz/44x6DLzW5Uc7w0/gkxscbusDUCzW+0i7QkRppDZ40SCPxhBylsMqFv2QOSkdpX7p0SUVleFLjDSHsGDKjqPsxx2rrRDJjoAPiSUl1po58c6c54jvGDRjczg8/zeajkke+9zRPNV/gPdXzXMkmWMvqPN8/zHP2CD0dMuX3eby6w2aX3mTjqT2DYKh91npz1FTKfNSholzj1X945WO89K0jPP7/ukKU55yS61z6scN0/0iFP/rxz7Oe1jnXn2Tln5xk+us310bcDCLwSX54m79w6stoLAMriK1Pzab4MueQyumaBI0oLALufG5QxTybWkVaNKaIRAYSalIQW8uqdt4/U2JA1/hMySH/4Oiv8oFjf/mOP29PcAOri80/8DSdY07DVrkqOfQbfbpPzzKYnUcl7hpufuMKplEjm6nCygY2SfZoADdAWQls7HX3wMU/9SR/78/+Yxoy5rnhUf7ff/wn8M+tMLX1AkiJrNd45fcf4GON14lExrmLMzz+a8vYy0u3/Mg7C3isdakOKZyoFbDYG5bgqnYLUa+jD0yS13zStkfnsYwnwytcztts6ZqLyCk9a1yk3zchGkkk0hF7IsfElAZJ3waj/lYlQySRTFd6vDYv3A7sfsBa7DBGZY4C94t0VGbUrl2zHFv6x/U7pb6nZH8S4+FLPfLvgaLk+TohzB4ECEK6xos3IS3EuJng2xAbo8otKUd+C3uCwlANAO2U/ipW9AchoqcQuRgZ1VGYeQKjTuo2VggtXPxZzD8icyktXQHUwyVaFp6HrNeQgcYXO9YPwZgYdJzdcQabzu7BpVg9fJFzMNjcEWjvEcbvqbJ4wUsseQWUp0dWFbteIyw3cwcw7GZ7lLDuHwaZOrfaZMyPaPQ63/mavJNvWviuApKJJqYRoasB1nPO7MYTmECy9i5BthjzaHOZQ8HmKHipqpTNzLXaCWVe6HTyXT3Ryp+uD5oczTnluHOrGGofKSwzYQ9vJmbjIwuu35gv6DyeM3Fwm4Vgk5YaUlEpZxaP4Q/adz5YKVlobfNUdJHMGrQV+OiiksqyYWBg3pk/l2OGdsacWEb+Sj6CtAgAEYWxZnEuMvuQOqLeCNYglELUGoj5GdJDbTpHJfG8xipL1hQMFxukDYnxBDKzRXWtQGiNjN+5geQtcQs9zttijBlRU5P0PnqS3smcmkj5ny7+AC+eWeDxNy6Qr66OPksIQW5rpFZxJZ9A9BSsb2LTW2uUbm8aK/tWWHtD5uaGYzhykO6JJlc+JvAODPjkqW/yh6M11nWNi9kU23mVhooJZTYyB8usYkPXMFbii4iwKEFvyHi0A02toqOjUcBTkykSw4ZJeXfrEi+//wDZl2vvaELahWta3ZvhEJVYhtoHlSGt6/kisYWR4ljlAWXe2o5+KmGKyVaRGA8pLFWZjnY2wljEQ+DhIpREhKHT39ixIKwoTR+5197MeLA0zLIWi2uwihKIINhbBqS4GW2WEWxlVFZ8elGF2poLXPLIuSrLDNKWK1MWufubNyj0OqYIcAxEG5asJpymx3+4BAGyXsMcO0S1nlARQbGpEG7iL5me4pqMMSO3ZYXFQKEJyTkerKCjvb0mSwdocMxTZhV+X5NVPYIgxytYmxvpdUb/L5hXcCJlbSS5lUhrCKXGK6om/b4l3Mrp5BUGOnTPLQPGEHQk35EWQLZbpE8usv5ERP+wRR+KCaOU0M+ZrA04Wt/k/X6fsGCPX+of5HPpSabDPg0vdvYbMqGlhs4PDDlKsbu5xJ2n0r/Mk5qwTCFZQSeL6ImAisr41NRz/JGZL6E+aGnLIVWZ8+XhEZbzFl/rHKOiUuoqwbyry5VDd6fjeqS5wtPBJrEFEMWcnqGwfC1eRApDWzpfI1UI5s1dzOBlVqBvPaoyo0VCKDxim46YL78Q6cdW8ZVkHtV7uDYpN4W1rpru0DxL3zVN8r0dPDWkZgW9zSrplODqhz2iNUG45YIdHQj0TMulSNMc7rWlSWmOWEo+intkpNt9u8CnIFDGn6dPLPDU33iBp4CfufQJun93kUc++7VdOQ6hFAQ+gcyJrc/z/SMEm+o6A80b4fbu22sPXipkrYqoVhC1Knqqga56pA2feELRPSIYLuRUZvo8Pr3OgUqHo9Ga888xVRpySORnXM3aSB0xUMHIQ+Ggv4XCkFpFTabUZMJX+idYTRs0vSEL4SbfWX0TYwUpsqjiykksHA7WeN/CRS7VTt0/D03rGIDcKEzRJKoURpYBDexUlI0/VorvZLHrGjncFiXBSpibl3rvBcqoXbBjODgW/Nzx5kg8PDoX0WywdaqCVRBuCCqrFlX0i5KZ+93lv13nYqHBi+2oxUbcdoGDip2AOWsY8lYFv91Cb3d22KQ99GoRtRrDQzWE6PJaltDVFTLrUWWH1i5TW0pYTNGwURZVLc6gz3LU6yFnYuRTjyEuXEF3OnsynjL9BLhy6+0Mr6EIPe28lK557k6Q4/7vFdOmC54sZiRsdmnjUlMntOt7BzvtOMoga+RJdZcYfuqDDGYUG09bxNSQVnPATK2Pr9yx1f2Eiioc5LUqdCeWlh/jCY2xgq52af916ngFQzzUAQZBw4t3mGZhixJugyc0TS9BCrOreqtrKoUrsWRd1PGFe8xYSc1LqKuEljdAyju/jmWthpyZYsp/g6pUXMnde7RlUhjHug1swA4TrhGjebDsh6Wt3OViDzvBTekOvtMRXTIwPi2ZUJOGDE1soW9C2nJIJAwZzvX5iWAZXb3HFbH3okt9GUQohc1yMBpv4SD54jTnf7BOOqERw4BaPSbLFAd+wyPY1njDDK+fIdKc4cE61hPEMxHBVoq/tI15G/bjjlH0tNrJ7Opr/nYbKFgn4Qes/+T76ByHdGuOc2/OMfMVxczrV5wFxPh5FRLheVS9BGMlX1o7RrB9ex93ewGPkIjAK/qzuF26mGihWzWyiYjeQkDaFCSTMDyY85Fn3uD9rXM8EV5mQ9cL8aOjXbu6QkMNCawmNj7GChLrEYqcqkpYVOuurYL1qYmUhoy5krQ4151iKurT8oYc8SzGGjJyruSGFElsFQv+Jh9tv8k/j+5hg82yUdn46bjGP4KxqqxrcbPHdkrs5XVM0K6g53botPsFIUZMzc2fc+s/l693/bcEQj4EbstCYOoRvQWBNwS/6/LdKjbI1KASgxpmhDUfHSrSpnI+PLFxfdEsWOFh/GJsEkzFkDU9gnYLOr17p+94JwgDBjNOWH8xb7veddekaMqU8LUod9oA0zJgstWnf2KCxvoW7FHA43pn7zA4Xi9FpSHCy0fC5GtRMlolymBnnAnK7e7eSs5c0nJtRVf5XlbefcSz/oRHPGd417vP0gyG1L0Urzj/Qx3gS01FpiTGJyuCHYml6Q1Hx1FWpLljM/gFawwQyHzUFDYUO4GNV8xhpRC7tNLoapf+LxlzgNh6aCsJZU5VprSUEysLfWfjFvUaeqJGyxvgo0isJRSahtT0jZuzHZOjR4UbxoqRJqcMdgylQeb1c8Z4B3VZ9DyMrc+MGOIDmTXEowouMXK697Ec9EII7nXAI9nd2PLOXjtiS5RCBAEYiwX07ASdY1Wmn11iexjR26iShR7p0Gfi350dFelY3DwbeU+QTkQMZ3z8vsR2+y54utcYBSFix1Ihy+9o/itT72sfyZg7uMWl1Qkmn1dM/NyXdvtdletGEQxWlPPmu7zeotG7vfP9tgFPWRJ55WNVkqcGNGoxUZAxV+1Rka6i6rgf40tNJ4sIZc5k0CezyvnjmGgkoBvokJ4OeWF7gUwr/uujv8aWqfIbW0/R8FxVQVdXqHkpH402UQgkkh+ceJHL9QkW/E18kfOluE3HRM5noXBonlVdZlSfg9U3+bno/rIIeSR5pnmJ7bxCT4d4cqfPy7U6nZue12LnZaxgoAO0717rOqw/BCyIlE6TcotDEYXc5frGocKlskS5QOAqmjznXr0nGN0sCm/hAL3DTYZHMrxND78nyKuutiOvus7n0UZEHrlS9OGcBQPeQGEC579jFaNxW2mRQ8nquyS9gwvM/MtNTLe7t0EdkM82WX+v4f1Tq866Xxh82KksLHx2yt9LOKGrW0S3dYXfGrZ5ZGKVL/xoi1Mrc4irtxYG3g+Uvb7G/y+3+6hhDU85PUiZxioDlbxgSUp2x1iJsaVo2T0nNR7GajzhESrnV2N8MIFiNakzGfR3VVEaz6UK7haH/80G+VSFyy8c540FQTyriQ70aVZjjrfWmQwGNL2YqtrGF3pUkVr25ZPCYiJBVSYcDdZG76twGp+GyAiEwRcU37VAW0sGrOqA2HpO+G19FxwU+pmyIXJifKoyJcGyntUYiIBtXUG/3mDmNeDP3f5Y8+MH2Hiyyrzntt8pkhBNVQjiQnO1pas0pOuLaKykjyItAqHx1JYb/+5FdPx6KOGKWQTrJqRfMGIawYK3SVumRELQNc692UO9I7buhngnOplSLiIVmBybpqjJCZhocfZTLZIpjXhlFtPKaUz1qf1ii4nn1m7pO2MliNxi+/375tIvPA954iiD4xMsf9Bn7qsZ1efOY9Y3dssXxja6IgxdMJelDH/wvay8x+PI4SusdOqc/B9T5NLV3cHOroIo63zUhCE2PnqpSrh9jwIeJlpsPFkhfnzI9596jUPhJi1vwLy3PTJ3Coo88nLWchUURXlg10QkxncmUjLDlzl14MJWm8Eg5NzCtCt7DTojnx1wniBrWpNaycC6mzISGVWRkKFYylujm3Pa66CkJUUR2ZzgJju9ewoBdRUzMAHSOKFjSbXDDoMjb5KfKnfOnjTXTeTvqAz8XuMu00+7TAqL8VgBxpNIf2/rQIVS6Lk2adPpsqyyGE8gCr+gtG0QRmI6oCNBXoG8VnS7zwV5zaIrBpEX7I4qND6ZIGu4GnVZr2HTdM8rInTkEcwOipLqopHtDb7SUs8z7hlVOoEbK9nQdabDHouL6+S1Sfa4BewOjEFYdx+N63tu+FS7w6SOi85MYXial0aLGIxyT9mKK2xFVebCzuh9R+7hUtwV+SqW1wkGNdq6jTeMCLYUg16DlVqNtakGUSWlXR0yXelT9VImg0Hh0+Uqz/yiwWZVpkRFvw9TWAhoK3axdeVjmfWIrc/VtE1Ph2xnFYbaJ9WKfIzxS3LPaZqEJdOKtV4NKd3n1s9D4+KdXc8mUmQ1sds4VmSFgL5sqqzxRT7Sje0Kasd+L13Ar2Uor/vMIsUVWx9jJSnOqT8SOZGw+DhGSFqBEnu0+bodlDKS2SkGR9skkxpb08h1H9nx6MVNDpzuo2/WFqO4No3vNtCOdbmDC/ZGc7+1zuV74QCmUSFvRojcbdI7ixHdwxLxdIe1uMm0OEL1iwl66+a5JqEkyJDuIUX6yJDVbo34ch376jfJk+SmmQChXEGNEpaBCQk2JUFvLF13i3Xr1gGPVHSfmuHZP/VNnqxd5kSw4qjBgkbs6gpds1MRFRY3YKI9BiZwKSsEociZ9jrMe9sseB3+4fnvpvWG5P+Z/QjvPnaRXzzxWd7KEi7mLbqmwrl0ml9c+yDnupMsbTU5MbPGgUqHtWoDgIEJCqo2K6orXKQ3IORCrnb52NwXWEYtLgCqKsUXLkVXBjllPnmXu3LhM5EUVG0ks6LE1hu9n/HYcYm2Bmv2+Ka011SOiRuwOjc63Te45nTFw2/UYH3jwad8ihtHViKWPtAgawgqF6TreC7Bi51JookMQkuqa5pu6EEFV6KeCKI1GPiga07AjHX6HZkK/L4gbRsSH7Lj8/hRSH72/IMd4zVI2x4/fOJ5FqMNBiYsdGZu8SwXFh9ddKm2o6a2pVairmK3cdEVHqtc5enDl/gnrU89PAEP7rurFQ1Fy8UadqpxxFjJ48iQEOfbo61EWkuOZJAH1FRaMDwC60kur7SRwnJkfr1IvxeizHeyIZEC2+nib27Tfi6hleUgXXmuqEQw2Saba7KyMEvaFAzmXZCdtTUoC57Zmc9LXY0R0POQiUDFApkI/D6oIajEEvScJi3o5MjUIDONHGaIQqdkhcD6CiEZfbdepjmyvuk6Uw9jbHr2jhkCkVtXwYggQ9M1DRoyHfnkBBgWfSdhUFhiROFt5jQ8420kyn8Du5PKK+GE966dRGrdhrlrIhSWjoloyphJb5NICHwhycaD34cRYyzR8sem2frOGLYDSCTekR6V/9Bg/h8+h7nFhkpYi1WCtCnQoUTeocO98Pwd/zlj3VqU56iFA7zxZw8w9dQqf/3Uv+GV4QKbeRUpLIeCTb6n9hovv2ueb/SP8a3/7Gn42os7bzpu/JokiEYDOT/D5tOaP/muL/Hp/+G7OPL8JjpNnUA7DDHDeKd7fPEeol7DtBtI1riUTjD1kqZyvujS9jab9FsGPPZDT7F9XPGu2kWksJxLp4tKJDNq4ln2O1HCugu3EEGu5k22dZW1rI4WjprsmgqrWnP48SUutCfxL4Y83znOJ+I/uPtkF4ZgFS/j1OwqR2vrTPiDUUAli4Zw4z24ylJaJQx5RaImJt7uO719XBNlCgsD7WTRZSrLnYPrd8ojEXMxYSqxU5JZlgCX/iBKmCJl8hDcjEJgvZtQvmWl1u2gLPHGLU7W9+56d3xPoBTxlECHFr8vXANQxqrNSlGqI2t2emnZsoLO/U3kAqsstp6j8fBiAcY1okxbPqq7d87EJbQveLS6RCBy0pvsjEf9i26wipeBjy80U16PSdVz6RypHqz7t7X0dUCo8lEl1QgCIm9nd5ffYoOw006j1PDsXIS5cYugEgULZi2m59NpudY2MW7z5hjLu78/7aDQwyiJCHxEUIQYQjpX9zTD3xxS1xYTKaJNHx0IsoqCordbeU2OV0LKrBRbg9QGlRYC+5xCaO/+WSUwUmFCtZNyHv0EitJ4oS1BNUBkGpFoVG8Ad6gB8df7NC77dLW7F0ZVgiPB8VhKlR0mahxlMUu5WdzR6+wYt44+r7jORy1ECn2aL/LCgFC40vhirbjfEH7gvuN6DeH7zi5ldQO9ufm2r/Xm5xi8+zCDAwIpLaaVYns+0ecaTL0c7+5rdoOScCsEVgnyKpjwzjfNVmvQJZM5FqhEAXo+4cnJJZ4JlvBFPjL/rcmULRMypXo8WbnE5058iPbWcfRbZ2/M1Bw+yIUfmUI2h3xu9SS1qxmsbOxUb+U3ZqVEFJE3QgyCtaRO/Xwfub61czffgrm7ZcCz9OEa3Ucz3h1d4GvD45yOZ5j1uzRUTCS3qMqEGemiTIWlKhMaIuOgJ3g17fJGOseFZBJwVRWp9djSVf7HU7/ExaNT/N3P/aeE25B8aZ7hlCRtgq5Y8pqlfXKDJ6eX+N6JV0Y0bcdUis8yRbC1M+lm1kOJDB9N2hAwO3Wrob0zWMcyje8YKSoJgF19tMqmoOVjsqzUKspo3bE7GlZi0aHTCN23KrPbhZTgqZFT8qhKq8RtBj3C7HhdWE9i/T02dPE84jkNFoKOcnOFdekso3Bd08sFZTwAglIR6AKeopN61EyIjcD0BFKDyCBpK/xOdO+sEe4Sxhe8OzrPUt5iNW+ONgd6V6rAEtzE2LJMnzRkzEFvk6NeSh4JZOBj4gcY8GhLJ42oqIyGp0daFnAC4oaXkBhFWgh53X21I+QdFy6XjKsnzSg4yo0kM2rkVlzGft62ojcRjs4DsBMY3CVMt+t2r9UqolaFyjWeYdYiOn28lQ3Ic8JqBYzB5jvne5cOTgh3rwa+26CU962v3P0mBSb0sJ5AhwrjC3QgyCuF50/RK07qnWBKR+4a9wa+u6a1pbpax+vdYZXPlWXq1rKhXbPXMjOQWY0utDVl4BEXwU5sgtEmelzQHBTf+Xhn9FLUXK4P5YZ3/HefvEibuUPKCk8efRcGh3cKWasgKhX0/BS6HhBPBzResnAbAY9enOXCDyhM3TFiM5NdlvsTHPjfn7uuiasrJpKOhSlZOOnu/7xq0XcR8JQbmmvjDRt4HJ7f4L3N8xzz68R2g7YcoLD0bcDpdJbD/gYnghW2Tkm84TSV0+dvyOj3Trb42I8/x1eWDvPGWwd4/OI6uvTaMa5T/I1gqxFZKyAziuW4gXj1LPlttnq55eoTbFm8LY+vDY8TW49Zv0tmFWt5nUvpBA0V01JDQpkRiJx5b5stDGdyxUvDRc7HLugIZM62dmZZsfFH5mdP/MmXuTpocWmjzcGJbRZqWxyKtmh5Q6a9bqHrKUy1irLQUpVf7jw1gsz4RbVCji8MyQQMj7Zv6wTcDYS1N8wl36h3VikUvZGeZ5yWLT0zTOA0JXsNUUykVo5XptjbCnKgSIOV/kMa7ENgPqzmZrFzk3hd6Sqr/J1dccnoYNx4dSAoNqOuCK9I61llsUXTUGEhSz1QlqxlUD2JNIK0AXndZ8/USkKgGg3ShuCUl9E1CRetT00m17grF+LlYnetkaNSZlno0pazFj939cN8ZOYMf2ziywxnBfbJE4gX3sQ+gEawwvOw/k4VlS8027rKRlpzokfpGoAaK8ivbUuAJVL5DdMXEsci58X9usu3p7iFow1Bbyp0zLKukBnlGse+Q7slqzVmMIAk2XGuHxsvSrl0ghTOO0UpRC0YBTSOeS01RQKUwEpZ0B7F8Yui+GHkdG7x+jlCG6e7KIIh68md+7vMkBXRgUoNIjOoRKOWNrGDwR2NUz9ymM0n6hz0tzDW6S36NqBvU+aVZIaUzA7JCiF1Oa87I1lXcSULq4QYb9e1CwX7M8byA0QItHQantj6znIBQ1Ccr9S6QEne4Jq45zg4RzpToz8fOvGwAVsJkFHk0lE3KmqQCm92mu3DVaZOrdMIHaHQ/fkFHnm9f8M0lmNgxrxvymtDCdIpQ1qXVO/w0O2Hn8GEimTCp/HaBubMBadL9BXHmuss+C5oWzVVLmZTXMnaDHTIelbjajjBhNfnsU+8yTePHOGxX9+9QVJTk5z9C49hnujx1yae59e/8G6OfiaH1ZuLr3cdW+ST1SRD7TPMfYI7KA65ZcDjDywqFixnzZGoeGBck7puHpEYb1QpVZY5aivomgqXkzYrcZ3ZqIcqL9pCr7KtK0yqPv/5/G9yWbf4nfYTnKosczxYYdHbJhCGLROwZSqs6zqxcRfvwARkY+6nuhDrlc1FY2NR0jFE8eSDYRLezgq9FOPBTnprvGtz+X+N8wy5bkLdw7J0q8RIpAnceSpr7HexcxLu1eHdMUStSt6IUKlwlVaFo4DQ7FRdGTHW/dy9brSjL4MiYSldF3Va6DpCAwOJsKBDgQ73IPVTQkhEs0FeETRlNGpMW8JVZ+3upwVu0R9nTcv7dqVT52qjRWYlWcOSTFeIlMQ+iMbLQo4W7TIV1dUR21k0WjA8qZHXWEGP32PGql1NREuUfkPGun51JQtb7lm8HohY7qSgrRixne8I1rrqlfzaEI2RdkH4HgiJzXO3+Sj/KVkEKcVBlNYRRZADrimuu+nc4+UWrExPiTjZeb8ieLKedOfTsHOPaotIM0SaYdY3XJB2B0hmInqLgrZyu++0WAMyC6F039e5bAaAGdXBR4+kEcCuXuopCmV3p8BKdmg8PVVWtpUd1wcmpCljfIRjd3AMp1+ymoL7dp/awCtYNRxjlpTn9+YRs1AKfWiG/rzi6cllOmmF1WGNqW9uYl547fY/vGwDVM/Q4Z3nC4YHIpKmpHdIAJM0ABGn9OYqLERbtOUAbQ0DU2FLV9nMaiTGIx+t8z7/6fxXSI2HnZuBjS1snCBbDeyhOeofWOMjB87QlgPCdUnl+QuY3u2xNNZXjqW0kiT3dm8s30aIfsuooLaU0j8YYazg7GCa7Syi5cc0/JiTlRW6OmI9q3EhmyA1Hq+pA66iQFgyo2j4CRWZUlEZLeVYG19ooiIqfyObJRIZPzbx9VGp5HPJIn0TciGZQglDKHLOx5N0s4hYuxOa6kLgawUNP6HqpTxaX96pBpjK2Tp1/6SVVohdtHqJG3VG3ykFvoEfz5inSGY8MqPQgdNe7DmUwgbebhflMnC5g0l/pH3JhaPTqwFyj6oj8ukGw7lwdOzWc/GkoSw33mGwjMdIw2NqGpN7O13RlR1VZ/nLAXnDoCYSTE9hFGQ1QdqQ1GemsNud6yjo+w0Z+AyePMDggEUJF0wnxgflWJ2yGkkW6apIaBpqWBQZFIxHIQCtypRn5q/wdOMSkdCkMzmdIz6R78ODGpexroEujs05N5zi7NYk87oDgl2pZGPliLVxhoFq9PvIZXnku6N23cOluaAOnJty43JOPOsRiXT0/qNg9z7B5rkLhq7VCY17tFyLa+6n64Ko8b9ZgynbA93O8cANex3dDraO+/jv22TR23J9tEzIwIZsG58ZZbiYG/72v/jDxIsp//77foaqECgEq8Y5Ibsm096o4sr58eywkiVSFAGahnTfk5QD5pVLX/306kc5Gq3xndF51s2Q/jUaLxXleAfnMWvr9/w+tS+9Qeh5hGWAY61jSW7hNK+mJnjtL/s8uXiGH5p8kf/2M3+YxV/XiAtv3PyDyu9m7JoR1qIDwbGFNZYnD93xsXcPKbafyPmF7/tfeSFZ5HQ8y1pSp+Ff5keaz9OSCVe1oWMmMFbywfoZ5tU2jwcpptiIhMIjOvS7/KW/8lO0XznE7Fe2eOsn2tSf2ODvPfYrXMkn+FvnfpTGeYteWb0x43UD5I2QeMoVGnTjkMb4H625ew2PvzogWgv5yvpRpqI+U2HflfoZj+WsST8P2UirpMZdnIF0VQ++yKkod/FVVEYks4JGdJF1ZtWIcoxEhkHS0REDE9I1EQMd0skjJxaUO9URUAYHikHmk2uFNpLUKJbTJjWV0PKGTjx6n4mRkvqHG6erYHeQs6sXUOnXU7i4ljogjSwm1Icg4Cn9gMq0TinYvV2Ms0FFoJRXJKrmEeyRz5D1ZNHV3O7ocARODOq5n6K8dsa/Ut8FOK62HsfsBC5g9bpOGBhVUvp+iPUFxoc8FIhK5ESqDzjgwffpLfjkEznbZkjfTI2qAMtUAbidsF8Y1wVFw9CSDUqtIrMeUhgORNu01cDtqhsZadt/4H5KZaNPJQzbaUR/GI4myPGgRQozmtTK+wvYxfBIO9Y8FFelpeSYLkiB8QTBdo5MvFEl6APFtZO/vb2WPrf99g9gOMaHRpQUgYsdFbw4wbKha31apw0yDfjFzjN8oHKW434HEPgYImmQ1hJrvxAtj8+n119/VaFH5NSqEVzJG/zWxUd5dHqCP9++6I6pCIqdz4+hVk1Ij83ix/E9v09tniM8j/RDj5FXFDoUVK/GeKtdzIXL19lWqFPH6Z+c4gPHTnOkusGXeyeoXJVUz6y5aqU7+WwhsBImwgFX7yLZUb+sGcw7T6PHwisc9VfZqlUJhKYqMzIka4UTeFUmzKjOTsBZBK5KCBbUNsefvsxbrVmGMxM0nlzjOw+eYUr1+VL/FG+8cogjK/ltBzsAJpDo0NkoZPk1bNkNjILHcctTYV56jcnwSU5/YwH5/kv80PSLPNc7wkZa40x3ikR7xLlHK4ypeo7JqaiMukqoStcNNxI5GsFAhyhhSIRjXjKrWMvqbnI1TqMzvjsLZM7QBiRGUfdSpsMe/Tykm4f0s4A49RkmPrmWDDKfRHscrm8yG3SQQ3nbVtPvBONtI8bLz2+o45G7o/rxyq6yU7qxYlcVxp6iKFUd6VgMu7uiv03MMtKJjtJZlrgt0IFP6Hl74lFjpRh5rGBBpkUpegjGK8wEixRXWcQjLKhQY3yF1AKZC9CCvKmxfUXrkiWdEBxsdnhjs4rOXNojqwtMq4bo9aH7YMcpooiNpy3zixu8lSlW82aRSnBlvqn1aMiYthq43kLYkc9VU8as6AZpoblTGB6tLjHjdcisZH6yw9WDkWssuweQWJYHDZLtCKsNVrg0XGkm6Dx5XAqqZJvLuQV26+bK+SY3TrtUUUUqJbToUBBsJXgD17qmNBMtHcP38fawEgKlWTdVQtFlUvWIRDZiZ7Z0lakvLtGca/Gzh38fz77rLX5q7gsc9LZpy5zDXp1LeY9VLaiKjGBXiqvUXrlm0oHQzKkA1x7F8Gu9U/y7zUexvz3JVx5vwrHfQRf6nbRonwKGx2eWeflDj7G4Ngm3MPC7W8jpKTb+Up+nZ6/wZP0q/+DL38Xk1+eY/9X+yB25xNLvm2PzPTn/6NC/5nPD4/zNL3ySw6/kN/faueUHu6D9YGWbl8I7F2jX/q+vMJ9/kF/4xIf5WOs13h1eAbYAV2SzqmtcyCYJhGbG63BQuXTn2cynKnOqwgI5hzzBbz7+b+Bx4PvhjazPlnF9/X576VFO/MuE4OzKTvsIeNvgRweSvAK9LCCJfTDXKqtv/vq3nbXkpVUO/0bE0toif/PEAR595DIHqh0eaa6wlVXYSqukWtHPQlLj4QlD5GUEMieUztYcGO2mPKl3OaGWE0mgdizRd0zPHOsRyQxtJd08QgpLI0gIVY6uS3ypiVTG4dom/TzkK5vHmHhFMP3N+2d/L6xrHqo8g38ju/NirLKoCABuSMWWUAXD44y5xjUje8f2iDAkmQh2XGXHtTywu1v6rhcWP+3u5wrt+k75fXtH0fy9hI4UWVVgPRfs+D1I2qAji/FcqkImTodjRl5I4Pk5SeCPKlpELlCTCbnnE/QU3kCwnbhqG+tb8tCQVz3XHX4P0ndCSexExpHmJjMqZdF3E3nJ0vTzkJpMOOX1+Mk3/ghnzs5Rf9On90jKb33iZ6jKjIHqsqIboxTXlOzTkppT7VWWZlq31CHcD5RzhhKGrX4Fte10F+W9uJ7UWBvWGWQ+2jhNTunBY+3u/49785RQ0pIbSSBzrHJpZbnVx+/vEOY7Zen3f7zfTmjLIW3pLA5uxJT555Y58m8WOf3VR/mvJh4beWMZr9hoaZxdx9hqVfb1A9foF3ZXkgYd8PqW+Rc7ZLXm6HUK63ynMAxtysnaKl95/Dj5F6q3/bXKdz/h5jBtEVmOyHIYDF06MklBa6zWhdmfM3GsqIxnq6f5xuOH+UblMDPPzaIGQ3Sngzc/R354ls0PZnzXE6/z2f7j/NzZD3Ho04raa6s3qaG8AXYZvrrRDHWAMHd3warE8npnlvc3zjIp4UweFJsgt0F6JFge2QV0rVdYCJiRblUCsbVczXtkuJYeXRMQW5+L2RTLWw1OnlvF3MKccDScss1GlmICgY4s3TjEDIqLQiqEUtg8e2cBj15ewVteYaHzNFtXa1ycadMMYo5W1otyTcvysEFiFEnqIYSln7ueMIHUBIVduyfG3VDdJFpRGX7Rpbjs+VJ2HN+VDhJmpHEBiFRGOxjgC8NQ+1RUxoFgm2/Fh3h9bZa5NxPs119625N417BjxoM3uJZc+bm7TEe+EWNl6CXG/z/O8FyTdL/XR3978BRZXTrBXRmElRIGO8by3EDIXLaWcL/vTE5eYvGG5vqI/AFBRwodCay0CCPwBpa05QTMJrBgBN6gqKZQO8ft+5rEM1ipRsxPoz6kA/gDgRp69JPAFbEpC5HGhN7etdIQgqCaMh30aQjJrOoBLliIrc8Vq6jKhANenbMvH2Thc5bGrz3Pxo8/Q+P7BVWbkxUbldIXpSUTGlJxuLJBszFwPdH2ABJLEgd4fVkYokFifHpZSCcO6Q9DjJaYsUneGkdLitKoT4ztI4RFAJ6vCZRmGPmjxZbtHl5siwm8qHx5WBjY3wOQOfTTwGnEpOdEyTfYIOZXlwivLt3QikN4nuvd2KgjfJ9d7rvlHJMkkGY3bGprgerTHyYbK4suF+mB1RwO1zlyaI28OnXbhpq94w3HEOfW9d1LNN5WBZHmyEEMaYbNMuwwhsAnz90Fc8Tr8EPTL9LwEk7PPE5tqYlMU8z8FFuP1nnfqTf5yZkv8o+WPsb6G1M88pnn0XfT8FMUonYL3TxE3GWFrEo0V7aa9A+ENGQwMnSsiZS2HNKS2SiFeCWvOE1gKTgvCjy0hW2r6I95LMXG59XhQdLNiPzS5Z0PvMVaJzzPBZKZY66MD0niI2KJtU6TJgK/6NR+8xDx1gHPuLvhK2eYPFdl8puTbE4f5lfe9SjbT2d86n3f5LHaEi01YN7fHtmE+0XjulJxP04pn0ln2So6/m7rKm8NZl23Xyvo5uGImi5Rlox204jUKIaZjyraMqy8NkOwJXluFepXNYtvbGEvvHmLLN69gypSVdk1FSJ+EcCVLqHAqJcRVlJewjsmWmNftHxINDyFeRlF5RgUgY7cYXbKgGC8ksu5Mttd/zeeK3WNNg0q1tg9CuKyuvN6ssoitCDsGPoHFbpmnIttJlCx53aUPqjY4gvBYBhAJneNHdxC6g0M3gB6vRAyiTACm7hUYDwbUVt/8AaENk5Q32zwm/ZR/ursb5OOTeUKy6TqURMp2nqoROANtQtCrduRZRbioicRuOqagfXY0ClfXjvG5qUW89nazT7+vsAxv0WFTqIIYwqBItQ8V7gQ+jmZVghhaVdixxBbQa5V0UPLvVfJ8EjhnJi1kQxTn4qfMRP0yNuaZMKDPEdoqEmXoi/ZH7M32bzfc1j49FXiV6b43M+c5FjzLFOqP2IBjLVkqLfd0Nk8x2qNSNPrxaiFEGncGO9G8GLL7w4j5r2Mtsy5mIcYkdGShlcGB7nw6jyPbPVuuwC1d0ChQ0jbFh0pTOBhAt8F0plAJhIVOz2gVZYsTvgP50/y0sYBri5NIFcDWovQPbRI9+gi2UzO3MIqqfH422d+hP4/O8iJt4aY0rbAD7FpevubX2sQSUa0pfnKt04yc/Hu5tvwrRXa/3KBX/rT7+PPty/yTDAks318IUmsoW92+rNtmarT/8khUWEPExUBrsS5ahs0kyrmjJnkl3/pO1l8+cZ9tq7DNeuhUa6fYd4LCLqlM6x0HlRvg9u+dU2/D/0+rK7iN5tMy5PoMOI3px6lXRtS91OONtapKNeNt0TLc86i23llZHj2emeO7dSlAPppwFan6mhn3GSGETu26SWs006gBSLbEdO23pREm4b6pQT/6pZzdbzPEBq6eURdJaDY5fOhCzGkQeAkIe5v4119zfhj124X70XZ6z2AzTL8To5sBW6hVwLDNRUq9ga/lwxQMQ7r70TkKrV4/cztzPcALvAqjrU4TleWasEzoJXz3BkP7AToTCJ0oa/yLNazhH6OUgbje+4702PfYxFt55XCM+UBw6YpzXOG5YMVfAFRURnpPLDKkvRyl7xzvMJaugWLqgsRvUYUKQCLL+D8+gSVyy4YeCAovGSMLcWmTjheEsAys5ztT7ER14hT3+X0haWvNNYKtJHoojJnPJU1Sm/hNlTxMGA7iFhN64hUoFKcBw47vcZGL3gI7s/fC7BXlqnEKT9/4Vk4DO+tnMMvmiufzxWvxAu3t4hbWxjq6esevx14ieWrgxN8tPYGM7IzcmQ+k2V8a2OBxhmJ7AxvO3UUdCw6cps4YYrLZMzOAOnakyAtVlro+sQDjyvbIWrTx+8J4inIKxZ7dEi7FtMIE95YniFZq/DoKz3UlXXyd7AxFLlx90hkMP7dzUG226X5RofXX5/jb8w+zYFgiynV49noomtOW8wtGkFNpIWjdU4ktAt4invMF4bAGnxrGBiPC9kU7bcMtbO31w7iWuSRwNTz62OE28CtA56bnHDd6aD+/bdY+LxC/Kx7CxGGnH3sEfKaR9r28AYGlRgGcz5YqF1NkZlBaIN3foVo2zkqRsDUHaY4rLUIIRxTUPb5MBb9gDxrVGp4Y2sGb8Iw4bvO8JlRaOSIjQplji/1Ls+gUtBcPhbJ7DoDQyvtQ0GZm/UNoq8n5PVHGc565FW3Y1FjWmOXrnKNNWEnfz76uydIy/S5hWg1Rp65jM4fhIHL9dCBwIQWmbmJKmlIl8oSIH2D0WIkWDZKkDVBR2D7Hmoo0RVBXjOoVsZiY8t5whyaI2sUaRPACpcus9KSVSXsgbO06fdp/dLXyaof4MCPVamJmJro8ZqZdgZvhcst7NgGgNNCvJbOMe9t0ZbJqJP2vNdhUmpmVRX/6w0WP712254Z7xRCKYwQxLnHUPujYyqDjmh5yDe/+Ag6Mljf4m8rRA5dVRsTzDuMbrVSeM8OSyk1rDUjvjBZp/W6onU2w6Ypwrq0XmZdC5h74sPzHwnMYICJE1p/5Tj/5P2fZPL//i844a/SkDl/6/IP85UzR3l0ePnmbyDETjl+qRm7drM0NufbYi24dt2K1lN+7qUPEb4r44Phq8yrHm9mM/y3Fz7Fyu8ssPizX7+jOWniF75WHN+ORxQ4nYmcmsC0amSTFdKmRx5JBrNq5POVtiBrGtShAfVqzGR1yPmVSXqvTnDw85ra6U3MW+fJC1PP0q/pjpFmpA3J//qxn+P/1vsp7qbvgN7ahu0Oj/wXAc9VJul+z3ew8ajiB378y3xP6xV+uBpTFrxrW7aYurndqraGf7B9hP/z8nuZ+OKl3emsW8E63WeZGRjMC55+9CLnNyfobRbzwW2u/Xc/GxuNNXrHbTVO8C+u4UUBYTVExBki1/gbzlZcbXZBO/2G3th8x1U6e6RscZ8tBXU/pekNaUjXYFEqQ0sNR88JrknpjYuVy0k7khmreZMV2aSlhk6/1MjIq3vPmVutMcOY6vk+2BqDGWf2ZIsqp500lxgFPKUx37h40O+B37VUNjXe8rYrr9yzlJYga2pUX450GrsWr2JcxnOLf7jpXK+TmXK3JvA7klxHvNWYZpgEVIR7vgo1OhMILfH6kmhD0Dw/RGz39mSsNs9pnU1591d/kj/5yBf5yebLzKguNZHSlwFVkaNEgA4sebVw8FWw4G0yo4ZUBcRFCmJSan659zi/cOEDtN/SsLp5x40k7xbC87BKOCKuEL0K3xTaMola3mLuqzW073oHeUMzqiZ0J2L8za75vssKxCLoyyqSrBrRPh0TrPQweQ6WkXAbcE7ogbw+vbKPG8NoWFkn2pxw7YdETltKpsMefrhjqii8Qn9xrd9P0UBZULY6uMXcMR7sFMESRiOHOWalxsV4kgxNVWg6OuKbLx9j7oK5Y8fwm/no2MyNR8YJQb+KH4XYUFFdCQunesjqirwiGZ6ukwd1lgJobFsqa4bamS1Y3XDC23cCa2GrQ2Vlkv/tyseIlt8By2wtNknQaUrj9S28foNfCz7ErzY/yF9o5/i1lDDMWWhtMxkOOFpdp+UNaakBbTUYpaG7usKGrvGzX/weGq/5VLZf2q3FehuWZ7zbe/2y5aUXjxAtKyauWKftMRaRZW8b+Ny7ldXoW0ZsD0FngXsGq6AVDpnw3ZcKLnh5LLjqtDuFI6yPoSbNdY4RiXUBUCQsZzJXszylevRNyOREn16j0H3cZpnefUFxoYuX3qR+OkJ87DHiCUXSdkGPCXbEm7t20WPaHZVCZdXQOBfjfes0eji8penW/UbaADGRYuNo1FqCokRdCEDYUbsJK6G6YpC5ZfuxHbuAaA3kVdhotoCimbpnCcKMYd9DaEGwJaguGfwXzqHv0J32XiJ84RwHfvoQP/fXPsRffP8ZDqoBA5nSNT4NadDWYCJDVnMiR+MJjnhDWjIgFB4ad+yzqsovXPgA9h/P0vraJfKy382DQOBj/bEu6Fi8QKMj9//84iVqFy/dk48aV1uNL7t6pD+02NDcXW+i/4ih1zcItl2boJo0NGWVxWiDdn3ogh2lkJUIM4yx4wHPaEG8Cw+ighmyBlQ/pXK1waVBm8QaGlKwoevMfFnRfuP2tTtvC6OdcPoa8fR4uFEuuLvM8grcyy2EXlsnOtfkha+dYObsPch8WIt++XWCl+Hwr+88rE4eI59psvbkYS5NC768qBGtlGotYbreJ5Aag2C9X2W7U+XUz2WIL3zt+rG+nZZrLCideLmD36/TOL2F3OqRF5svk966QgvuZcDzHxH8Xs43LyzyWnWW0NP0hiHWQiV0kbkoxJAAae4VZbE78YvWcvS95JmHThR+JcNagf9KlYk3iwt0ryq0xmDzDDMw1L9xgXoYYAMfWwnQtYCsGaBDQR7tLAD+wKCGmmA7RQwzZG+A7fUxw+EDYwVuhiP/ZoP88xVUr+Ms9pMUW4vQFZ/hgcilXi9sYwIPXfEIL2wg+kMeWZoBa5H9BHIN1jL9gpuy1FoXW4vI2xEySRC5QQ5SRH+I7nb3dMxmu4v3+kWmfuYoHz7y5+kdEmR1Sz6V057t8v75i7Rf8mi/uo1NM2pLGT/28h9nqjKg6qV87aUTBKuK1mloXEyJ3riEWX2wYmWz3cXfGHJmeYK1bo2Xw3nEW1Ua52++076XqK6k/Hcv/iiNSoInDdXzHvVL5rYp9H04+Msd/vJn/yiF5yWVq4rKmsX0ll0Jd5re83tlxAalGUHHshHX2DJOkL+SNguD0b2fY+8X7NUVTv6LKmqtc98IB3t1BW9zm7nlpuukXg+xvsL4EcarkBXM1mRmmE4N3usX33FgJ09forVcx/Z6u4OcW1RnldgPeO4CIjXk3YDu0KMrLaLvgYHEG7t5iuBGDWQhkt3J8wgtRlS6MuBpXBrLCmpXLJW1vdG4XIdCOW/znPzq0uhhWa3it5qomTa6FpI1/VE0F2wlqE4MV1exwyH5g3YZvgXMS2+ipHCVHzDqoSOEoHHquHvO+Ut47Rbe9ASsrJN3OlCMfdftVHiBlY+VIZ+99nl7CJul6PUN1O9uMCEVzY89w3A2oHPEZ3vY4jlpqK5qNyFqjdfNWLowyWqjge9r2i95tM5mhJ95zjG4ezQGOUyx/QZDLUgTn8q2INzWI1Hx/WRCVT9jeLVOOulTqSYEXQi6+m0rg/axG2IQ03pdIXKLP4DKeoa/7XRSAFbfvyBSaINMIdGK2CoGxvVl/HaH6ffhGy/f1/t2VMy0vjF6TOBYrRsl0u7F3Ki3tuE2vHtuBLFXJcL72Mc+9rGPfexjHw8K+8nofexjH/vYxz728W2P/YBnH/vYxz72sY99fNtjP+DZxz72sY997GMf3/bYD3j2sY997GMf+9jHtz32A5597GMf+9jHPvbxbY/9gGcf+9jHPvaxj3182+P/DzvNx2efSXGwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the image above is : [9 0 0 3 0 2 7 2 5 5]\n"
     ]
    }
   ],
   "source": [
    "# visualization some of the data\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the image above is :', y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99e81f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ba72d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing : (n, 28, 28) --> (n, 784)\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# one_hot_encoding on Y:         (a simple function for this purpose: to_categorical)\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cd2dbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10), (10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "031edd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN creating: \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# a 5-layer model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape = (784, ))) # input layer\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50)) # Activation layer , \"the wheight is Randomized [it's the best]\"\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50)) # Activation layer\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50)) # Activation layer\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10)) # Output layer\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "sgd = optimizers.SGD(lr = 0.01) # \"lr is the Learning Rate\", slower learning: lr = 0.001\n",
    "\n",
    "model.compile(optimizer = sgd,\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa645845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3107 - accuracy: 0.1125\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2935 - accuracy: 0.1470\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2836 - accuracy: 0.2096\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2664 - accuracy: 0.2803\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2218 - accuracy: 0.3241\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.0632 - accuracy: 0.3287\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.7810 - accuracy: 0.3649\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.5959 - accuracy: 0.4406\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.4770 - accuracy: 0.4756\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3806 - accuracy: 0.4826\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2895 - accuracy: 0.5190\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2165 - accuracy: 0.5376\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1387 - accuracy: 0.5648\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0930 - accuracy: 0.5832\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0471 - accuracy: 0.6095\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0113 - accuracy: 0.6291\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9859 - accuracy: 0.6330\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9783 - accuracy: 0.6417\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9690 - accuracy: 0.6409\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9465 - accuracy: 0.6438\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9204 - accuracy: 0.6551\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9036 - accuracy: 0.6665\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9199 - accuracy: 0.6626\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8940 - accuracy: 0.6751\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8960 - accuracy: 0.6751\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8874 - accuracy: 0.6698\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8684 - accuracy: 0.6770\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8794 - accuracy: 0.6710\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8651 - accuracy: 0.6743\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8545 - accuracy: 0.6691\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8919 - accuracy: 0.6630\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8414 - accuracy: 0.6804\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8231 - accuracy: 0.7027\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8269 - accuracy: 0.6947\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8081 - accuracy: 0.7025\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8272 - accuracy: 0.6906\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8043 - accuracy: 0.6983\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7847 - accuracy: 0.7082\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8432 - accuracy: 0.6919\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8416 - accuracy: 0.6873\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8056 - accuracy: 0.6964\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7994 - accuracy: 0.6926\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7887 - accuracy: 0.7037\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7640 - accuracy: 0.7158\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7703 - accuracy: 0.7165\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7783 - accuracy: 0.7120\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7572 - accuracy: 0.7135\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7479 - accuracy: 0.7244\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7408 - accuracy: 0.7226\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7643 - accuracy: 0.7206\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7873 - accuracy: 0.7032\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7712 - accuracy: 0.7034\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7677 - accuracy: 0.7079\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7604 - accuracy: 0.7070\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7527 - accuracy: 0.7188\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7331 - accuracy: 0.7192\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7979 - accuracy: 0.6977\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7832 - accuracy: 0.7006\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7837 - accuracy: 0.6990\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7495 - accuracy: 0.7179\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7450 - accuracy: 0.7183\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7283 - accuracy: 0.7275\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7532 - accuracy: 0.7187\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7494 - accuracy: 0.7209\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7528 - accuracy: 0.7249\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7703 - accuracy: 0.7210\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8319 - accuracy: 0.6856\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7682 - accuracy: 0.7074\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7664 - accuracy: 0.7174\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7352 - accuracy: 0.7154\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7689 - accuracy: 0.7030\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7627 - accuracy: 0.6974\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7499 - accuracy: 0.7073\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7273 - accuracy: 0.7115\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7509 - accuracy: 0.7128\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7323 - accuracy: 0.7181\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7483 - accuracy: 0.7104\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7506 - accuracy: 0.7217\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7343 - accuracy: 0.7181\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7400 - accuracy: 0.7106\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7278 - accuracy: 0.7212\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7107 - accuracy: 0.7268\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7099 - accuracy: 0.7368\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6968 - accuracy: 0.7215\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7313 - accuracy: 0.7238\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7321 - accuracy: 0.7305\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7274 - accuracy: 0.7262\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7186 - accuracy: 0.7349\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7022 - accuracy: 0.7276\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6928 - accuracy: 0.7305\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6901 - accuracy: 0.7360\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6892 - accuracy: 0.7375\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.7471\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6875 - accuracy: 0.7391\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6935 - accuracy: 0.7405\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6855 - accuracy: 0.7405\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6903 - accuracy: 0.7497\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7094 - accuracy: 0.7463\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7442 - accuracy: 0.7247\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7321 - accuracy: 0.7362\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(x = x_train, y = y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "385579f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 800us/step - loss: 0.7129 - accuracy: 0.7312\n",
      "\n",
      " Test Accuracy is: 0.7311999797821045\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print('\\n Test Accuracy is:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec550514",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the model\n",
    "## solution 1 : Wheight Initialization => NOT IMPROVING results in our CASE\n",
    "def slt1_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(50, input_shape = (784, ))) # input layer\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(50)) # Activation layer , \"the wheight is Randomized [it's the best]\"\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(10)) # Output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    sgd = optimizers.SGD(lr = 0.01) # \"lr is the Learning Rate\", slower learning: lr = 0.001\n",
    "\n",
    "    model.compile(optimizer = sgd,\n",
    "                 loss = \"categorical_crossentropy\",\n",
    "                 metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ac7a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3121 - accuracy: 0.1129\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2925 - accuracy: 0.1635\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2828 - accuracy: 0.2162\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2647 - accuracy: 0.2990\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2164 - accuracy: 0.3429\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.0466 - accuracy: 0.3319\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.7778 - accuracy: 0.3745\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 1.6399 - accuracy: 0.4575\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 1.5519 - accuracy: 0.4814\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 1.4414 - accuracy: 0.5108\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3069 - accuracy: 0.5413\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2072 - accuracy: 0.5569\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1323 - accuracy: 0.5747\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0764 - accuracy: 0.5781\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0386 - accuracy: 0.5984\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 1.0028 - accuracy: 0.6271\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 0.9695 - accuracy: 0.6542\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 0.9363 - accuracy: 0.6712\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9272 - accuracy: 0.6690\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9031 - accuracy: 0.6739\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8902 - accuracy: 0.6813\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 0.8862 - accuracy: 0.6836\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8809 - accuracy: 0.6664\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8653 - accuracy: 0.6831\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8846 - accuracy: 0.6747\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8827 - accuracy: 0.6756\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8700 - accuracy: 0.6778\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8460 - accuracy: 0.6915\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8669 - accuracy: 0.6810\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8904 - accuracy: 0.6700\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8767 - accuracy: 0.6778\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8362 - accuracy: 0.6931\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8965 - accuracy: 0.6695\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8698 - accuracy: 0.6769\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8313 - accuracy: 0.6942\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8612 - accuracy: 0.6809\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8443 - accuracy: 0.6875\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8296 - accuracy: 0.7017\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8308 - accuracy: 0.6928\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8742 - accuracy: 0.6661\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8938 - accuracy: 0.6674\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8481 - accuracy: 0.6729\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8397 - accuracy: 0.6780\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9199 - accuracy: 0.6539\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8437 - accuracy: 0.6869\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8683 - accuracy: 0.6661\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 0.8289 - accuracy: 0.6899\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8220 - accuracy: 0.6921\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9084 - accuracy: 0.6529\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8109 - accuracy: 0.6921\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7842 - accuracy: 0.7143\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8071 - accuracy: 0.7009\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7909 - accuracy: 0.7139\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8568 - accuracy: 0.6837\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8259 - accuracy: 0.6935\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7862 - accuracy: 0.6964\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7962 - accuracy: 0.6967\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7702 - accuracy: 0.7112\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7734 - accuracy: 0.7132\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7588 - accuracy: 0.7224\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8617 - accuracy: 0.6399\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8558 - accuracy: 0.6265\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8510 - accuracy: 0.6355\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9111 - accuracy: 0.6207\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8041 - accuracy: 0.6855\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9100 - accuracy: 0.6375\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8034 - accuracy: 0.6955\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7818 - accuracy: 0.7058\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8150 - accuracy: 0.6959\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7997 - accuracy: 0.6912\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7960 - accuracy: 0.6891\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7762 - accuracy: 0.7060\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7874 - accuracy: 0.7099\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7709 - accuracy: 0.7192\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7913 - accuracy: 0.6941\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7695 - accuracy: 0.7059\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7508 - accuracy: 0.7182\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7672 - accuracy: 0.7208\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7595 - accuracy: 0.7120\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7461 - accuracy: 0.7094\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7688 - accuracy: 0.7102\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7529 - accuracy: 0.7096\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8243 - accuracy: 0.6837\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7745 - accuracy: 0.6864\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7450 - accuracy: 0.7123\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7301 - accuracy: 0.7224\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7266 - accuracy: 0.7315\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7184 - accuracy: 0.7292\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7410 - accuracy: 0.7281\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7514 - accuracy: 0.7237\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7448 - accuracy: 0.7063\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7969 - accuracy: 0.6872\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7376 - accuracy: 0.7258\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7617 - accuracy: 0.7061\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7919 - accuracy: 0.6823\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7329 - accuracy: 0.7211\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7109 - accuracy: 0.7286\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7387 - accuracy: 0.7339\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7173 - accuracy: 0.7267\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.7082 - accuracy: 0.7331\n"
     ]
    }
   ],
   "source": [
    "wheighted_model = slt1_model()\n",
    "history = wheighted_model.fit(x = x_train, y = y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7208a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the model\n",
    "## solution 2 : Non-Linearity / activation -> 'relu'\n",
    "def slt2_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(50, input_shape = (784, ))) # input layer\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer , \"the wheight is Randomized [it's the best]\"\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10)) # Output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    sgd = optimizers.SGD(lr = 0.01) # \"lr is the Learning Rate\", slower learning: lr = 0.001\n",
    "\n",
    "    model.compile(optimizer = sgd,\n",
    "                 loss = \"categorical_crossentropy\",\n",
    "                 metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df8820fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1001\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000A: 0s - loss: nan - \n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.1000\n",
      "Epoch 98/100\n",
      " 303/1200 [======>.......................] - ETA: 0s - loss: nan - accuracy: 0.0991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-08bb1fcec818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnon_linear_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslt2_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_linear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "non_linear_model = slt2_model()\n",
    "history = non_linear_model.fit(x = x_train, y = y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59d485a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the model\n",
    "## solution 3 : Batch Normaliztion (our old nrmalization !) --> Perfect\n",
    "#(especially if you have 'relu', use 'Batch_Normalization')\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "def slt3_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(50, input_shape = (784, ))) # input layer\n",
    "    model.add(BatchNormalization()) # Normalization added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer , \"the wheight is Randomized [it's the best]\"\n",
    "    model.add(BatchNormalization()) # Normalization added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(BatchNormalization()) # Normalization added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    #model.add(BatchNormalization()) # Normalization Not added, its arbterary / Hyper Parameter\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10)) # Output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    sgd = optimizers.SGD(lr = 0.01) # \"lr is the Learning Rate\", slower learning: lr = 0.001\n",
    "\n",
    "    model.compile(optimizer = sgd,\n",
    "                 loss = \"categorical_crossentropy\",\n",
    "                 metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9219be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.8091 - accuracy: 0.7410\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.4779 - accuracy: 0.8316\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.4258 - accuracy: 0.8482\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3954 - accuracy: 0.8589\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3737 - accuracy: 0.8654\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3584 - accuracy: 0.8703\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3423 - accuracy: 0.8755\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3329 - accuracy: 0.8784\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3219 - accuracy: 0.8828\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3125 - accuracy: 0.8857\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3074 - accuracy: 0.8891\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2976 - accuracy: 0.8918\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2928 - accuracy: 0.8931\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8951\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2825 - accuracy: 0.8962\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2800 - accuracy: 0.8972\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2747 - accuracy: 0.8992\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.9020\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2643 - accuracy: 0.9038\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2598 - accuracy: 0.9042\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2559 - accuracy: 0.9058\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2520 - accuracy: 0.9068\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2478 - accuracy: 0.9091\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2447 - accuracy: 0.9096\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.9105\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2399 - accuracy: 0.9111\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2354 - accuracy: 0.9139\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2316 - accuracy: 0.9139\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2312 - accuracy: 0.9147\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2264 - accuracy: 0.9163\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2261 - accuracy: 0.9173\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2234 - accuracy: 0.9166\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2213 - accuracy: 0.9179\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2148 - accuracy: 0.9201\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2125 - accuracy: 0.9221\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2119 - accuracy: 0.9212\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2090 - accuracy: 0.9226\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9214\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2071 - accuracy: 0.9222\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2047 - accuracy: 0.9229\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9263\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9256\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9268\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9272\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9281\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9282\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9305\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9297\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9296\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9312\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9312\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9334\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9328\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9337\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9334\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9340\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1757 - accuracy: 0.9340\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9347\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1709 - accuracy: 0.9355\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1695 - accuracy: 0.9366\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1690 - accuracy: 0.9375\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1689 - accuracy: 0.9374\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1660 - accuracy: 0.9380\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1669 - accuracy: 0.9375\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1633 - accuracy: 0.9387\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1618 - accuracy: 0.9406\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1606 - accuracy: 0.9406\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1634 - accuracy: 0.9384\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1621 - accuracy: 0.9398\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1595 - accuracy: 0.9404\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1574 - accuracy: 0.9409\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1557 - accuracy: 0.9427\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1553 - accuracy: 0.9423\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1557 - accuracy: 0.9419\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1512 - accuracy: 0.9425\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1525 - accuracy: 0.9437\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1510 - accuracy: 0.9438\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1501 - accuracy: 0.9441\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1491 - accuracy: 0.9447\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1480 - accuracy: 0.9447\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1479 - accuracy: 0.9446\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1466 - accuracy: 0.9451\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1461 - accuracy: 0.9462\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1443 - accuracy: 0.9473\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1441 - accuracy: 0.9464\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1405 - accuracy: 0.9473\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1415 - accuracy: 0.9480\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1432 - accuracy: 0.9463\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1399 - accuracy: 0.9467\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1386 - accuracy: 0.9483\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1404 - accuracy: 0.9475\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1373 - accuracy: 0.9492\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1377 - accuracy: 0.9484\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1363 - accuracy: 0.9488\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1354 - accuracy: 0.9496\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1327 - accuracy: 0.9506\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1337 - accuracy: 0.9486\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1348 - accuracy: 0.9498\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1325 - accuracy: 0.9516\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1313 - accuracy: 0.9506\n"
     ]
    }
   ],
   "source": [
    "normalized_model = slt3_model()\n",
    "history = normalized_model.fit(x = x_train, y = y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d1833ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 991us/step - loss: 0.4633 - accuracy: 0.8699\n",
      "\n",
      " Test Accuracy is: 0.8698999881744385\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "results3 = normalized_model.evaluate(x_test, y_test)\n",
    "print('\\n Test Accuracy is:', results3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "16ecd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the model\n",
    "## solution 4 : Dropout -> Randomly neglect some neurons (e.g. 20%) in each layer\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "def slt4_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(50, input_shape = (784, ))) # input layer\n",
    "    model.add(Dropout(0.2)) # Dropout(0.2 = 20% of nerons in this layer) added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer , \"the wheight is Randomized [it's the best]\"\n",
    "    model.add(Dropout(0.2)) # Dropout(0.2) added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    model.add(Dropout(0.2)) # Dropout(0.2) added\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(50)) # Activation layer\n",
    "    #model.add(Dropout(0.2)) # Dropout(0.2) Not added, its arbterary / Hyper Parameter\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10)) # Output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    sgd = optimizers.SGD(lr = 0.01) # \"lr is the Learning Rate\", slower learning: lr = 0.001\n",
    "\n",
    "    model.compile(optimizer = sgd,\n",
    "                 loss = \"categorical_crossentropy\",\n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1e3692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 19.7961 - accuracy: 0.0975\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0943\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 2.3027 - accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0961\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0996\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0997\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.1004\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0964\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0967\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0965\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0963\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0998\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0962\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0963\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 92/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0967\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0973\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3027 - accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "dropeed_model = slt4_model()\n",
    "history = dropeed_model.fit(x = x_train, y = y_train, batch_size = 50, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
